{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a875c01-bb42-4bbc-a118-5c0c9ba2ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.sparse import lil_matrix,csr_matrix, save_npz, load_npz\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7acaa1f-5eb2-4476-9a69-e114d5e13fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the folders for the project\n",
    "output_path = 'output'\n",
    "input_path = 'input'\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    \n",
    "if not os.path.exists(input_path):\n",
    "    os.makedirs(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ee1b6a-1d7d-4182-8cb9-faef338d770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset if not existing\n",
    "# !wget -nc http://files.grouplens.org/datasets/movielens/ml-20m.zip\n",
    "# !unzip -n ml-20m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f897f51-618b-4249-b934-2fd603999c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing step\n",
    "try:\n",
    "    # Try reading the preprocessed file if existing, if not generate\n",
    "    ratings_df = pd.read_csv(os.path.join(input_path, 'ratings_preprocessed.csv'))\n",
    "except:\n",
    "    # Read the original file\n",
    "    ratings_df = pd.read_csv(os.path.join('ml-20m', 'ratings.csv'))\n",
    "    # Make the userId start from zero\n",
    "    ratings_df['userId'] = ratings_df['userId'] - 1\n",
    "\n",
    "    # Create a mapping for movieId since they are not sequential\n",
    "    unique_movie_ids = set(ratings_df['movieId'].values)\n",
    "    movie2idx = {}\n",
    "    for i, movie_id in enumerate(unique_movie_ids):\n",
    "        movie2idx[movie_id] = i\n",
    "    # Add them to ratings_df\n",
    "    ratings_df['movie_idx'] = ratings_df.apply(lambda row: movie2idx[row['movieId']], axis=1)\n",
    "\n",
    "    # No need the timestamp of the rating\n",
    "    ratings_df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "    # Save the new ratings_df\n",
    "    ratings_df.to_csv(os.path.join(input_path, 'ratings_preprocessed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42085d5-5f35-4c39-9af3-a99be03a6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the subset data from the generic dataset to adapt the training to the hardware size\n",
    "# n = 10000\n",
    "# m = 2000\n",
    "\n",
    "# user_id_count = Counter(ratings_df['userId'])\n",
    "# movie_id_count = Counter(ratings_df['userId'])\n",
    "\n",
    "# user_ids = [user for user, count in user_id_count.most_common(n)]\n",
    "# movie_ids = [movie for movie, count in movie_id_count.most_common(m)]\n",
    "\n",
    "# ratings_lite_df = ratings_df[ratings_df['userId'].isin(user_ids) & \n",
    "#                              ratings_df['movieId'].isin(movie_ids)].reset_index().copy()\n",
    "\n",
    "# Generate a new mapping adapted to the number of users to adapt the size of the sparse matrix\n",
    "# new_user_id_map = {old: i for i, old in enumerate(user_ids)}\n",
    "# new_movie_id_map = {old: j for j, old in enumerate(movie_ids)}\n",
    "\n",
    "# ratings_lite_df.loc[:, 'userId'] = ratings_lite_df.apply(lambda row: new_user_id_map[row['userId']], axis=1)\n",
    "# ratings_lite_df.loc[:, 'movieId'] = ratings_lite_df.apply(lambda row: new_movie_id_map[row['movieId']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c031eaee-613b-4771-8d0a-55d64d2c0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_lite_df = ratings_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b998738b-342c-459a-824e-333a4d9bc489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138493, 131263)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = ratings_lite_df['userId'].max() + 1\n",
    "M = ratings_lite_df['movieId'].max() + 1\n",
    "\n",
    "N, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df930687-af8b-4edc-aa62-f517c5fa9ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_matrix(N, M, df):\n",
    "    A = lil_matrix((N, M))\n",
    "    \n",
    "    def update_sparse_matrix(row):\n",
    "        i = int(row['userId'])\n",
    "        j = int(row['movieId'])\n",
    "        \n",
    "        A[i, j] = row['rating']\n",
    "        \n",
    "    df.apply(update_sparse_matrix, axis=1)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02c5e10d-400a-48d0-a347-f4f63b271edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sparse matrix for train and test\n",
    "# lil better for adding new values, csr better for saving\n",
    "df = shuffle(ratings_lite_df)\n",
    "\n",
    "cut_off = int(0.8 * len(df.index))\n",
    "df_train = df.iloc[:cut_off]\n",
    "df_test = df.iloc[cut_off:]\n",
    "\n",
    "A_train = make_sparse_matrix(N, M, df_train)\n",
    "A_train = A_train.tocsr()\n",
    "mask_train = (A_train > 0)\n",
    "save_npz(os.path.join(input_path, 'A_train.npz'), A_train)\n",
    "\n",
    "A_test = make_sparse_matrix(N, M, df_test)\n",
    "A_test = A_test.tocsr()\n",
    "mask_test = (A_test > 0)\n",
    "save_npz(os.path.join(input_path, 'A_test.npz'), A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1453f799-fad3-4bc3-9832-cb5fd729c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "regularization = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0be33388-fbe8-457b-a4a1-519399c88512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.525713787506539\n"
     ]
    }
   ],
   "source": [
    "# global rating  = Sum of ratings / number of ratings \n",
    "mu = A_train.sum() / mask_train.sum()\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7e64db8-0476-47c0-abc0-047a3fe45525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    # The real loss should not consider those rows where the rating is missing\n",
    "    # we need to use a mask to gather the ratings to calculate a real loss.\n",
    "    mask = K.cast(K.not_equal(y_true, 0), dtype='float32')\n",
    "    diff = y_pred - y_true\n",
    "    sqdiff = diff * diff * mask\n",
    "    sse = K.sum(K.sum(sqdiff))\n",
    "    n = K.sum(K.sum(mask))\n",
    "    \n",
    "    return sse / n\n",
    "\n",
    "def generator_train(A, M):\n",
    "    while True:\n",
    "        A, M = shuffle(A, M)\n",
    "        #  For each step in the nmber of batch steps\n",
    "        for i in range(A.shape[0] // batch_size + 1):\n",
    "            # Make the moving upper step\n",
    "            upper = min((i+1) * batch_size, A.shape[0])\n",
    "            # Crop the batch to generate from the A matrix\n",
    "            a = A[i*batch_size: upper].toarray()\n",
    "            m = M[i*batch_size: upper].toarray()\n",
    "            # Subtract the global average rating to center the data (working with deviations over mu) \n",
    "            a = a - mu * m\n",
    "\n",
    "            yield a, a\n",
    "\n",
    "def generator_test(A, M, A_test, M_test):\n",
    "    while True:\n",
    "        A, M = shuffle(A, M)\n",
    "        #  For each step in the nmber of batch steps\n",
    "        for i in range(A.shape[0] // batch_size + 1):\n",
    "            # Make the moving upper step\n",
    "            upper = min((i+1) * batch_size, A.shape[0])\n",
    "            # Crop the batch to generate from the A matrix\n",
    "            a = A[i*batch_size: upper].toarray()\n",
    "            m = M[i*batch_size: upper].toarray()\n",
    "            at = A_test[i*batch_size: upper].toarray()\n",
    "            mt = M_test[i*batch_size: upper].toarray()\n",
    "            # Subtract the global average rating to center the data (working with deviations over mu) \n",
    "            a = a - mu * m\n",
    "            at = at - mu * mt\n",
    "\n",
    "            yield a, at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a995739-a279-43ea-abf3-2261daff841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(M,))\n",
    "x = Dropout(0.7)(i)\n",
    "x = Dense(700, activation='tanh', kernel_regularizer=l2(regularization))(x)\n",
    "x = Dense(M, kernel_regularizer=l2(regularization))(x)\n",
    "\n",
    "model = Model(i, x)\n",
    "model.compile(\n",
    "    loss=mse_loss,\n",
    "    optimizer='adam',\n",
    "    metrics=[mse_loss],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0717c19-99ee-4a93-82ac-7bb705292856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1082/1082 [==============================] - 520s 480ms/step - loss: 0.8376 - mse_loss: 0.6863 - val_loss: 1.1705 - val_mse_loss: 0.9829\n",
      "Epoch 2/20\n",
      "  62/1082 [>.............................] - ETA: 3:30 - loss: 0.8099 - mse_loss: 0.6233"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    generator_train(A_train.copy(), mask_train.copy()),\n",
    "    validation_data=generator_test(A_train.copy(), mask_train.copy(), A_test.copy(), mask_test.copy()),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=A_train.shape[0] // batch_size + 1,\n",
    "    validation_steps=A_test.shape[0] // batch_size + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8019f1-8393-4a52-93df-3fead1e0b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60bb41-83ca-433c-ab88-56e01326a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['mse_loss'], label='train mse')\n",
    "plt.plot(history.history['val_mse_loss'], label='test mse')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
